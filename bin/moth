#!/usr/bin/env python3

import lex
import parse
import sys
import os
import resolve_imports
import find_user_types

def parse_flags(args):
    i = 0
    to_compile = ""
    output_file = "a.out"
    cc = "g++-12"
    keep_temp = False
    flags = ["-o","-keep_temp","-cc","-I","-L","-l"]
    includes = []
    links = []
    while i < len(args):
        currentFlag = ""
        val = args[i]
        for flag in flags:
            if args[i].startswith(flag):
                currentFlag = flag
        if currentFlag in ["","-keep_temp"]:
            i += 1
            if currentFlag == "":
                to_compile = val
            elif currentFlag == "-keep_temp":
                keep_temp = True
        else:
            val = args[i][len(currentFlag):].strip()
            if val == "":
                val = args[i+1]
                i += 2
            else:
                i += 1
            if currentFlag == "-o":
                output_file = val
            if currentFlag == "-cc":
                cc = val
            if currentFlag == "-I":
                includes.append("-I" + val)
            if currentFlag == "-L":
                links.append("-L" + val)
            if currentFlag == "-l":
                links.append("-l" + val)
    return {"filename": to_compile,"output":output_file,"keep_temp":keep_temp,"cc":cc,"includes":includes,"links":links}

if __name__ == "__main__":

    args = sys.argv[1:]

    token_file = "/".join(os.path.realpath(__file__).split("/")[:-1]) + "/tokens.txt"
    header_file = '#include "' + "/".join(os.path.realpath(__file__).split("/")[:-1]) + '/moth_lib.hpp"' + "\n"
    
    if len(args) == 0:
        print("USAGE: moth inputfile [flags]")
        exit()

    flags = parse_flags(args)

    f = flags["filename"]

    tmp_file = f.split(".")[0] + "_moth_tmp.cpp"

    out_file = flags["output"]

    with open(f,"r") as f:
        raw = f.read()
    original_len = len(raw.split("\n"))
    #print(original_len)
    raw,headers,tmp = resolve_imports.resolve(raw)
    new_len = len(raw.split("\n"))

    raw = lex.remove_comments(raw)

    raw = lex.convert(raw)

    after_all = len(raw.split("\n")) - 2

    line_offset = after_all - original_len

    user_types = find_user_types.find_types(raw)
    statics = find_user_types.find_static(raw)
    dtypes = find_user_types.find_dtype(raw)

    lexer = lex.get_lexer(token_file,user_types,statics,dtypes,line_offset=line_offset)

    tokens = lexer.lex(raw)

    #for token in tokens:
    #    print(token)
    #exit()

    parser = parse.get_parser(token_file,line_offset=line_offset)

    base = parser.parse(tokens)

    out = header_file + headers + base.eval(line_offset=line_offset)

    with open(tmp_file,"w") as f:
        f.write(out)
    os.system(flags["cc"] + ' ' + tmp_file + ' -o ' + out_file + " " + " ".join(flags["includes"]) + " " + " ".join(flags["links"]))

    if not flags["keep_temp"]:
        os.remove(tmp_file)




